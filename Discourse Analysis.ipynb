{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4KK56ST7b37",
        "outputId": "046c82e4-12c9-4415-8bb0-e2dc89848417"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f9AtDR5yTRp",
        "outputId": "143ee4f8-5fcb-4b44-fa73-a77c2d66571b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences:\n",
            "Natural Language Processing is a subfield of artificial intelligence.\n",
            "It deals with the interaction between humans and computers using natural language.\n",
            "\n",
            "Words (after removing stopwords):\n",
            "['Natural', 'Language', 'Processing', 'subfield', 'artificial', 'intelligence', '.', 'deals', 'interaction', 'humans', 'computers', 'using', 'natural', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is a subfield of artificial intelligence. \" \\\n",
        "       \"It deals with the interaction between humans and computers using natural language.\"\n",
        "\n",
        "# Tokenize the text into sentences and words\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords (common words like \"is\", \"a\", \"the\" that don't carry much meaning)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Perform basic discourse analysis\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nWords (after removing stopwords):\")\n",
        "print(filtered_words)\n",
        "\n",
        "# You can perform more advanced discourse analysis, like co-reference resolution,\n",
        "# discourse parsing, or coreference resolution depending on your specific goals.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
